{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DebbyLeung/rl_tutorial/blob/main/finetune_tut.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tEfKx0C3-Sc0"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "!pip install tensorflow==2.14.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bfgCf-VCuq0"
      },
      "source": [
        "Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MExsKdrG7Zq3"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "dataset = load_dataset(\"glue\", \"cola\")\n",
        "dataset_train = dataset[\"train\"]  # Just take the training split for now\n",
        "dataset_sentence = dataset_train[\"sentence\"]\n",
        "dataset_label = dataset_train[\"label\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenized datasets"
      ],
      "metadata": {
        "id": "OlnKrVm4A0NT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7p4XewD5Eb0S"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "import numpy as np\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "tokenized_data = tokenizer(dataset_sentence, return_tensors=\"np\", padding=True)\n",
        "# Tokenizer returns a BatchEncoding, but we convert that to a dict for Keras\n",
        "tokenized_data = dict(tokenized_data)\n",
        "labels = np.array(dataset_label)  # Label is already an array of 0 and 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iES6-e0lCPBb",
        "outputId": "4cd56d09-2228-4d81-9772-bb3e0f6ee865"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import TFAutoModelForSequenceClassification\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load and compile our model\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\")\n",
        "model.compile(optimizer=Adam(3e-5))  # No loss argument!\n",
        "# model.fit(tokenized_data, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nykNAACEqgT"
      },
      "source": [
        "Branch 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(tokenized_data, labels)"
      ],
      "metadata": {
        "id": "HxCKG4Hjh7-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "TkMpTConEog6"
      },
      "outputs": [],
      "source": [
        "model_path = \"ft-bert-base-cased\"\n",
        "# model.save_pretrained(model_path)\n",
        "model.save(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8Cyp_L8EtuN"
      },
      "source": [
        "Branch 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llIg9BSSCARb"
      },
      "outputs": [],
      "source": [
        "def tokenize_dataset(data):\n",
        "    # Keys of the returned dictionary will be added to the dataset as columns\n",
        "    return tokenizer(data[\"text\"])\n",
        "dataset = dataset.map(tokenize_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1q0B4_zGROa"
      },
      "outputs": [],
      "source": [
        "tf_dataset = model.prepare_tf_dataset(dataset[\"train\"], batch_size=16, shuffle=True, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UAyEtaCGcy8"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=Adam(3e-5))  # No loss argument!\n",
        "model.fit(tf_dataset)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "mount_file_id": "151WWKXcNw8_VwP9TdWE0aTRDFi-N4iP3",
      "authorship_tag": "ABX9TyNFiyMDwhBMauy09p/2RBHt",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}